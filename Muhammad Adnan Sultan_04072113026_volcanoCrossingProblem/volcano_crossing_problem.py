# -*- coding: utf-8 -*-
"""Volcano Crossing Problem

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lo2dcdgxw0qSW4oVk7YY5tduLqKUjwhc

# Volcano Crossing Value Iteration and Policy Iteration

This project implements value iteration and policy iteration algorithms to solve the volcano crossing example. The goal is to find the optimal path to cross the volcano safely by maximizing the total reward. The project also includes a visualization of how the values change after every iteration of the algorithms.

## Project Structure

- `main.py`: Main script containing the implementation of value iteration and policy iteration algorithms.
- `README.md`: This readme file.

## Requirements

- Python 3.6+
- NumPy
- Matplotlib
- Seaborn

Install the required libraries using pip:

```sh
pip install numpy matplotlib seaborn
"""



import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Define the grid and rewards
grid_size = (5, 5)
rewards = np.zeros(grid_size)
rewards[4, 4] = 1  # Goal state

# Initialize value function and policy
values = np.zeros(grid_size)
policy = np.random.choice(['up', 'down', 'left', 'right'], size=grid_size)

# Define the possible actions
actions = ['up', 'down', 'left', 'right']

# Define the transitions and rewards
def get_next_state(state, action):
    x, y = state
    if action == 'up':
        return max(0, x - 1), y
    elif action == 'down':
        return min(grid_size[0] - 1, x + 1), y
    elif action == 'left':
        return x, max(0, y - 1)
    elif action == 'right':
        return x, min(grid_size[1] - 1, y + 1)

def value_iteration(values, rewards, discount=0.9, theta=0.0001):
    delta = float('inf')
    while delta > theta:
        delta = 0
        new_values = values.copy()
        for x in range(grid_size[0]):
            for y in range(grid_size[1]):
                v = values[x, y]
                value_list = []
                for action in actions:
                    next_state = get_next_state((x, y), action)
                    reward = rewards[next_state]
                    value_list.append(reward + discount * values[next_state])
                new_values[x, y] = max(value_list)
                delta = max(delta, abs(v - new_values[x, y]))
        values = new_values
        visualize(values)
    return values

def policy_iteration(policy, values, rewards, discount=0.9):
    is_policy_stable = False
    while not is_policy_stable:
        values = policy_evaluation(policy, values, rewards, discount)
        is_policy_stable = True
        for x in range(grid_size[0]):
            for y in range(grid_size[1]):
                old_action = policy[x, y]
                action_values = {}
                for action in actions:
                    next_state = get_next_state((x, y), action)
                    reward = rewards[next_state]
                    action_values[action] = reward + discount * values[next_state]
                best_action = max(action_values, key=action_values.get)
                if old_action != best_action:
                    is_policy_stable = False
                policy[x, y] = best_action
                visualize(values)
    return policy, values

def policy_evaluation(policy, values, rewards, discount=0.9, theta=0.0001):
    delta = float('inf')
    while delta > theta:
        delta = 0
        new_values = values.copy()
        for x in range(grid_size[0]):
            for y in range(grid_size[1]):
                action = policy[x, y]
                next_state = get_next_state((x, y), action)
                reward = rewards[next_state]
                new_values[x, y] = reward + discount * values[next_state]
                delta = max(delta, abs(values[x, y] - new_values[x, y]))
        values = new_values
        visualize(values)
    return values

def visualize(values):
    plt.figure(figsize=(6, 6))
    sns.heatmap(values, annot=True, cmap='viridis', cbar=False, square=True)
    plt.show()

# Run the value iteration algorithm
values = value_iteration(values, rewards)
print("Optimal Values after Value Iteration:")
print(values)

# Run the policy iteration algorithm
policy, values = policy_iteration(policy, values, rewards)
print("Optimal Policy and Values after Policy Iteration:")
print(policy)
print(values)